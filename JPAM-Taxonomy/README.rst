JPAM Taxonomy
=============


There is a great deal of latent information buried within publications, beyond obvious categorical tags. As one example, topic models and word embeddings are common techniques used to associate related texts via latent features. We explored these types of approaches for associating new contextual information with articles.  Beyond simply deriving these often opaque tags we explore new methods for categorizing these articles through the development of an automatically generated topic tree for our target journals. We explored a variety of topic modeling and clustering techniques in order to generate a document taxonomy that was representative of all of the articles in the journals. 

Our first attempt at creating a taxonomy for the journals used the Hierarchical LDA model developed by David Blei, a form of unsupervised learning that aims to discover latent topics present in a corpus of papers, as well the relationships between the latent topics. Preprocessing of data consisted of mapping each document into a bag of words format, which essentially ignores order of words in the learning process, and pruning stop words, or words which have little importance to the meaning of an article, such as the words “the” and “of”. Overall, the algorithm, for our purposes, was infeasible due mainly to two problems. First, it is computationally intensive and therefore slow.  Secondly, and more importantly,  our corpus size was too small to develop sufficiently coherent topics. Nonetheless, the results made some sense, but we saw little improvement with greater iterations or increased data. 

Our next attempt consisted of constructing a vanilla LDA model on the corpus. We preprocessed the data in an identical manner to the above approach. Ultimately, the approach yielded more intelligible topics than the hierarchical variety. Following the construction of the base vanilla LDA model, we built a tree on the topics using hierarchical clustering. Before clustering, we filtered out topics we viewed as noise by hand, such as “doi journal public policy”, that could be considered “catch all” topics. We would expect almost every article in all of the journals to fall into these topics, so we gain no benefit by keeping them in the model. For the clustering, we used a measure of closeness that considered the pairwise distance between two given topics in the LDA model, where each topic is a probability distribution over all of the words found in our corpus, minus the words we exclude through stopword pruning.  This approach generated a tree of height log(n), where n is the number of topics we generated, and log is the base 2 logarithm. In this tree, the original topic distributions were leaves. 

One of the biggest problems with such latent models is that topics are opaque: they are a collection of words that are often meaningless to others. To address this problem and in an effort to derive metadata to be associated with documents we wanted to provide single word or phrase labels to each topic. To tackle this problem, we developed and tested a myriad of labeling approaches. Specifically, we applied wiki-labeling approaches that leverage wikipedia’s vast corpus of curated documents to obtain labels. We will mention some of the more successful wiki-labeling approaches here. We used a direct wiki-labeling technique to label the leaves of our developed tree which consisted of finding the wikipedia article returned with search of several of the most probable words in a given topic. After we used this technique to label the leaves, we tested a number of metrics that given a set of wikipedia pages and topics could find good labels to apply to intermediate nodes of the tree (e.g. where two or more topics meet, a most recent ancestor). Some of the techniques we used are as follows:

* Shortest Common Ancestor in the wikipedia category tree
* Shortest Common Ancestor guided by A*
* Most central node in graph of wikipedia links
* Highest pagerank among wikipedia category pages above the topic pages
* Highest pagerank among pages linked to by topic pages

These techniques left some unlabeled intermediate nodes or nodes that have the same label as their parent. In these situations, we combined such unlabeled or similarly labeled nodes to produce a more compact tree. To help visualize our results, we created dendrograms to see the exact hierarchical clustering (see Section 7). This approach labels the leaf nodes, but not the intermediate ones, as the produce a more readable graph. We also tested a circular tree so that intermediate node labels could be visualized.
